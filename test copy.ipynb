{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hope well\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\houss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\houss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append(\"C:/Users/houss/AppData/Roaming/nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Prétraitement des données\n",
    "def preprocess(text):\n",
    "    # Convertir le texte en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = re.sub(r\"[^a-z0-9]\", \" \", text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Supprimer les stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Exemple de prétraitement\n",
    "text = \"Hello, how are you doing? I hope you're doing well.\"\n",
    "print(preprocess(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour comment allez esp re allez bien\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Ajouter le chemin des données nltk si nécessaire\n",
    "nltk.data.path.append(\"C:/Users/houss/AppData/Roaming/nltk_data\")\n",
    "\n",
    "# Télécharger les ressources nécessaires si absentes\n",
    "try:\n",
    "    stopwords.words(\"french\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "try:\n",
    "    word_tokenize(\"test\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# Initialiser les stopwords et le stemmer pour le français\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "# Fonction de prétraitement\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # mise en minuscules\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)  # garder les caractères français\n",
    "    tokens = word_tokenize(text, language=\"french\")  # tokenisation\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]  # stopwords + stemming\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Exemple de prétraitement\n",
    "texte = \"Bonjour, comment allez-vous ? J’espère que vous allez bien!\"\n",
    "print(preprocess(texte))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['Bonjour', 'Comment', 'ça', 'va', 'Cest', 'un', 'exemple', 'de', 'texte', 'pour', 'la', 'tokenisation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\houss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Exemple de texte\n",
    "texte = \"Bonjour ! Comment ça va ? C'est un exemple de texte pour la tokenisation.\"\n",
    "\n",
    "# Nettoyage simple : suppression de la ponctuation\n",
    "texte_nettoye = re.sub(r'[^\\w\\s]', '', texte)\n",
    "\n",
    "# Tokenisation\n",
    "tokens = nltk.word_tokenize(texte_nettoye, language='french')\n",
    "print(\"Tokens :\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour', 'Comment', 'ça', 'va', 'Cest', 'exemple', 'texte', 'tokenisation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\houss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "tokens_filtres = [mot for mot in tokens if mot.lower() not in stop_words]\n",
    "print(tokens_filtres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la France LOC\n",
      "Massy LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "texte = \"Houssam Alrachid est le président de la France. Il travaille à Massy.\"\n",
    "doc = nlp(texte)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "texte_en = \"I love this product, it is amazing!\"\n",
    "blob = TextBlob(texte_en)\n",
    "print(\"Sentiment :\", blob.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0\n",
      "Confusion matrix :\n",
      " [[0 1]\n",
      " [0 0]]\n",
      "Classification report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\houss\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\houss\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\houss\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\houss\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\houss\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\houss\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "texts = [\n",
    "    \"J'adore ce produit, il est fantastique !\",\n",
    "    \"C'est terrible, je suis très déçu.\",\n",
    "    \"La qualité est excellente et le service impeccable.\",\n",
    "    \"Je n'aime pas du tout, c'est une perte de temps.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1: positif, 0: négatif\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "pipline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "pipline.fit(X_train, y_train)\n",
    "y_pred = pipline.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix :\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report :\\n\", metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "corpus = [\n",
    "    \"Le chat dort sur le canapé.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"La souris mange du fromage.\",\n",
    "    \"Le chat et le chien jouent ensemble.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Prétraitement : tokenisation des phrases\n",
    "sentences = [word_tokenize(phrase.lower()) for phrase in corpus]\n",
    "\n",
    "# Entraînement du modèle Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Exemple : afficher l'embedding du mot \"chat\"\n",
    "print(\"Embedding de 'chat' :\", model.wv['chat'])\n",
    "\n",
    "# Trouver les mots les plus similaires à \"chien\"\n",
    "print(\"Mots similaires à 'chien' :\", model.wv.most_similar('chien'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Pour TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Pour Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Exemple de corpus en français\n",
    "corpus = [\n",
    "    \"Le chat dort sur le canapé.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Le chat et le chien jouent ensemble.\",\n",
    "    \"La souris mange du fromage dans la cuisine.\"\n",
    "]\n",
    "\n",
    "# --- Étape 1 : Prétraitement du corpus ---\n",
    "def preprocess(text):\n",
    "    # Mise en minuscule, suppression de la ponctuation et tokenisation\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "corpus_clean = [preprocess(doc) for doc in corpus]\n",
    "\n",
    "# --- Étape 2 : TF-IDF ---\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus_clean)\n",
    "print(\"Matrice TF-IDF :\\n\", tfidf_matrix.toarray())\n",
    "print(\"\\nDimension de la matrice TF-IDF :\", tfidf_matrix.shape)\n",
    "print(\"Mots dans le vocabulaire TF-IDF :\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# --- Étape 3 : Entraînement du modèle Word2Vec ---\n",
    "# Pour Word2Vec, on doit fournir une liste de listes de tokens.\n",
    "tokenized_corpus = [doc.split() for doc in corpus_clean]\n",
    "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=2, seed=42)\n",
    "# Exemple : affichage de l'embedding pour le mot \"chat\"\n",
    "print(\"\\nEmbedding de 'chat' (Word2Vec) :\", w2v_model.wv['chat'])\n",
    "\n",
    "# Exemple : mots similaires à \"chien\"\n",
    "print(\"\\nMots similaires à 'chien' (Word2Vec) :\")\n",
    "for word, similarity in w2v_model.wv.most_similar(\"chien\", topn=3):\n",
    "    print(f\"{word}: {similarity:.2f}\")\n",
    "\n",
    "# --- Étape 4 : Discussion ---\n",
    "print(\"\\nComparaison :\")\n",
    "print(\"- TF-IDF génère une représentation sparse, de dimension égale à la taille du vocabulaire (ici, par exemple, 20 mots).\")\n",
    "print(\"- Word2Vec génère des vecteurs denses (ici de dimension 100) qui capturent la similarité sémantique entre les mots.\")\n",
    "print(\"- TF-IDF est utile pour des tâches basées sur la fréquence et la différenciation des mots, tandis que Word2Vec est plus adapté pour capturer des relations sémantiques fines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Préparation du jeu de données\n",
    "# Jeu de données fictif : textes et étiquettes (0 ou 1)\n",
    "texts = [\n",
    "    \"Ce produit est excellent et de grande qualité.\",\n",
    "    \"Je n'aime pas ce produit, très décevant.\",\n",
    "    \"Service client au top, très satisfait.\",\n",
    "    \"Expérience médiocre, je ne recommande pas.\",\n",
    "    \"Produit fantastique et livraison rapide.\",\n",
    "    \"Mauvaise qualité, très déçu.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de validation\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Chargement du tokenizer BERT (exemple en anglais, adapter selon la langue si nécessaire)\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Création d'un Dataset personnalisé\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze pour retirer la dimension inutile\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Instanciation des datasets\n",
    "train_dataset = TextClassificationDataset(texts_train, labels_train, tokenizer)\n",
    "val_dataset = TextClassificationDataset(texts_val, labels_val, tokenizer)\n",
    "\n",
    "# 2. Configuration et entraînement du modèle\n",
    "# Chargement du modèle pré-entraîné pour la classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Définition des métriques d'évaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Configuration de l'entraînement avec Trainer de Hugging Face\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Data collator pour gérer le padding dynamique\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "trainer.train()\n",
    "\n",
    "# 3. Évaluation du modèle\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Résultats d'évaluation :\", eval_results)\n",
    "\n",
    "# Sauvegarde du modèle fine-tuné\n",
    "model.save_pretrained(\"custom_bert_model\")\n",
    "tokenizer.save_pretrained(\"custom_bert_model\")\n",
    "print(\"Modèle sauvegardé dans le dossier 'custom_bert_model'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1,2,3,1,3,6,5]\n",
    "set1=set()\n",
    "res=set()\n",
    "for i in list:\n",
    "  if i in set1:\n",
    "    res.add(i)\n",
    "  else:\n",
    "    set1.add(i)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.random.randn(10000000)\n",
    "plt.hist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"pythoninterviewquestion\"\n",
    "\n",
    "def first_recurring(input_str):\n",
    "  \n",
    "  a_str = \"\"\n",
    "  for letter in input_str:\n",
    "    a_str = a_str + letter\n",
    "    print(a_str)\n",
    "    print(a_str.count(letter))\n",
    "    if a_str.count(letter) > 1:\n",
    "      return letter\n",
    "  return None\n",
    "\n",
    "first_recurring(input_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import date, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings par phrase :\n",
      "tensor([[-0.3939, -0.1698,  0.3645,  ..., -0.0601, -0.0722, -0.0484],\n",
      "        [-0.1869, -0.4071, -0.1066,  ..., -0.1215, -0.3370,  0.4853]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Charger le tokenizer et le modèle BERT pré-entraîné\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Exemple de textes à encoder\n",
    "sentences = [\n",
    "    \"Hello world!\",\n",
    "    \"This is an example of generating embeddings with BERT.\"\n",
    "]\n",
    "\n",
    "# Tokenization et création des tenseurs d'entrée\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Passer les entrées dans le modèle sans calcul de gradients\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Récupérer les embeddings de la dernière couche (shape : [batch_size, seq_length, hidden_size])\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Agrégation simple pour obtenir un embedding par phrase (par exemple, moyenne sur la dimension des tokens)\n",
    "sentence_embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "# Affichage des embeddings\n",
    "print(\"Embeddings par phrase :\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7475\n",
      "Epoch 2 - Loss: 0.7436\n",
      "Epoch 3 - Loss: 0.6664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification #, Adamw\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Exemple de Dataset\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Données fictives\n",
    "texts = [\"I love this product\", \"I hate this service\"]\n",
    "labels = [1, 0]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TextClassificationDataset(texts, labels, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Chargement du modèle BERT avec une tête de classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Boucle d'entraînement simplifiée\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 1, 'h': 1, 'l': 3, 'd': 1, 'r': 1, 'w': 1, 'o': 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_by_char(string):\n",
    "    freq = {}\n",
    "    string = string.replace(\" \", \"\")\n",
    "    unique_chars = set(string)\n",
    "    for char in unique_chars:\n",
    "        freq[char] = string.count(char)\n",
    "    return freq\n",
    "\n",
    "filter_by_char(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import isprime\n",
    "def is_prime_number(list_int):\n",
    "    prime_list = []\n",
    "    for num in list_int:\n",
    "        if isprime(num):\n",
    "            prime_list.append(num)\n",
    "        else :\n",
    "            pass\n",
    "    return prime_list\n",
    "is_prime_number([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_pangram(string):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for char in alphabet:\n",
    "        if char not in string.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "is_pangram(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world Hello'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_words(string):\n",
    "    words = string.split()\n",
    "    reversed_words = words[::-1]\n",
    "    return \" \".join(reversed_words)\n",
    "\n",
    "reverse_words(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_dupplicates(list_):\n",
    "    return list(set(list_))\n",
    "remove_dupplicates([1,2,3,4,5,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def missing_int(list_):\n",
    "    missing = []\n",
    "    for int_ in range(0, max(list_)+1):\n",
    "        if int_ not in list_:\n",
    "            missing.append(int_)\n",
    "    return missing\n",
    "\n",
    "missing_int([1,2,3,4,6,7,8,9,10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 1, 'o': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_vowels(string):\n",
    "    vowels = \"aeiuo\"\n",
    "    fre = {}\n",
    "    string = string.lower().replace(\" \", \"\")\n",
    "    for vow in vowels:\n",
    "        if vow in string:\n",
    "            fre[vow] = string.count(vow)\n",
    "    return fre\n",
    "\n",
    "count_vowels(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9047375096555625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def area_of_triangle():\n",
    "    a = float(input(\"Enter the first side of the triangle: \"))\n",
    "    b = float(input(\"Enter the second side of the triangle: \"))\n",
    "    c = float(input(\"Enter the third side of the triangle: \"))\n",
    "    s = (a+b+c)/2\n",
    "    area = np.sqrt(s*(s-a)*(s-b)*(s-c))\n",
    "    return area\n",
    "\n",
    "area_of_triangle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare(list1, list2):\n",
    "    list_1 = sorted(list1)\n",
    "    list_2 = sorted(list2)\n",
    "    if len(list_1) != len(list_2):\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(len(list_1)):\n",
    "            if list_1[i] != list_2[i]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "compare([1,2,3,3,5], [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def validate_email(email):\n",
    "    pattern = r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "    if re.match(pattern, email):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "validate_email(\"john.doeexample.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fibonacci(N):\n",
    "    fib = [0, 1]\n",
    "    if len(fib) == N :\n",
    "        return fib\n",
    "    elif len(fib) > N:\n",
    "        return [0]\n",
    "    else:\n",
    "        while len(fib) < N :\n",
    "            fib.append(fib[-1]+fib[-2])\n",
    "    return fib\n",
    "\n",
    "fibonacci(10)\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aidnI ym evol I'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'I love my India'[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 1,\n",
       " 'e': 1,\n",
       " 'D': 1,\n",
       " 'a': 2,\n",
       " 't': 3,\n",
       " ' ': 1,\n",
       " 'S': 1,\n",
       " 'i': 2,\n",
       " 's': 1,\n",
       " 'c': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class element_count:\n",
    "    def __init__(self, string):\n",
    "        self.string = string\n",
    "        self.freq = {}\n",
    "        \n",
    "    def char_count(self):\n",
    "        unique_chars = set(self.string)\n",
    "        for char in unique_chars:\n",
    "            self.freq[char] = self.string.count(char)\n",
    "        return self.freq\n",
    "    \n",
    "\n",
    "count = element_count(\"Data Scientist\")\n",
    "res = count.char_count()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 12, 0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def move_zeros_to_end(arr):\n",
    "    for num in arr :\n",
    "        if num == 0:\n",
    "            arr.remove(num)\n",
    "            arr.append(num)\n",
    "    return arr\n",
    "\n",
    "move_zeros_to_end([0,1,0,3,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def get_input():\n",
    "    '''Function takes range from to to'''\n",
    "    from_ = int(input('Number From: '))\n",
    "    to = int(input('Number To: '))\n",
    "    return from_, to\n",
    "\n",
    "def is_prime(from_, to):\n",
    "    '''Function finds prime numbers in the given range'''\n",
    "    for num in range(from_, to + 1):\n",
    "        for i in range(2, num):\n",
    "            if (num % i) == 0:\n",
    "                break\n",
    "        else:\n",
    "            print(num)\n",
    "\n",
    "def main():    # Taking input from user\n",
    "    from_, to = get_input()\n",
    "    # Finding prime numbers\n",
    "    is_prime(from_, to)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(from_,to):\n",
    "    for num in range(from_,to):\n",
    "        for i in range(2,num+1):\n",
    "            if num%i == 0:\n",
    "                break\n",
    "        else:\n",
    "            print(num)\n",
    "def main():\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic(sorted(dic.items(), key=lambda x: x[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
